# ðŸ§ª AI Prompt Injection Lab â€” 2025 Portfolio Project

This lab demonstrates modern, real-world prompt injection vulnerabilities and defenses, with hands-on examples using LLMs and Flask.

---

## âœ… Completed Modules (so far)

### ðŸ”¹ Setup
- [x] Kali Linux environment with Python virtualenv
- [x] Flask app running locally
- [x] Basic OpenAI API config prepared (GPT-4 tested)

### ðŸ”¹ Attacks Implemented
- âœ… HTML comment injection (`<!-- ignore rules -->`)
  - ![Demo](images/attacks/html-comment-injection-demo.png)
- âœ… JSON-form bypass attempt (Safe Mode OFF)
  - ![Demo](images/attacks/json-form-injection-safe-off.png)

### ðŸ”¹ Defenses Tested
- âœ… JSON input filtering with Safe Mode ON
  - ![Blocked](images/defenses/json-form-injection-safe-on.png)

---

## ðŸ“· Image-Backed Reports

All attacks and defenses have dedicated `.md` reports with screenshots in:

- `/attacks/`
- `/defenses/`
- `/images/`

---

## ðŸ”„ Next Phase (In Progress)
- Live API test with OpenAI GPT-4
- Memory context leakage exploration
- Detection and alerting mechanisms (AI firewall)
